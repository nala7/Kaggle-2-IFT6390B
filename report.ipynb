{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe0a060-9554-483b-98c9-55609a843223",
   "metadata": {},
   "source": [
    "# Second Kaggle Competition Report - IFT6390B\n",
    "**Students:** Nadia Gonzalez Fernandez and Yorguin José Mantilla Ramos\n",
    "\n",
    "**Team:** \"Nadia + Yorguin\"\n",
    "\n",
    "\n",
    "## Milestone 1\n",
    "\n",
    "### Feature Design\n",
    "#### PCA\n",
    "\n",
    "*Resource*: https://www.askpython.com/python/examples/principal-component-analysis\n",
    "\n",
    "This code implements a custom Principal Component Analysis (PCA) function from scratch. This allows for dimensionality reduction by transforming the original features into a new, smaller set of uncorrelated features that capture the most variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ba11d1-0aff-4961-9a4e-caa76692db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, n_components):\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    n_samples = X.shape[0]\n",
    "    cov_matrix = np.dot(X_centered.T, X_centered) / (n_samples - 1)\n",
    "\n",
    "    # Compute eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Select top components\n",
    "    components = eigenvectors[:, :n_components]\n",
    "\n",
    "    X_reduced = np.dot(X_centered, components)\n",
    "\n",
    "    return X_reduced, components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed482e4-bc58-489b-ad60-70d6415df7ea",
   "metadata": {},
   "source": [
    "#### Stratified Sampling\n",
    "\n",
    "This function implements a stratified sampling technique designed to handle imbalanced datasets while maintaining the proportional representation of classes. It calculates the number of samples to draw from each class based on their original distribution, ensuring that the sampling reflects the original class ratios. The method distributes the total desired sample size across classes, taking into account the available samples in each class and avoiding over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1231fe37-d0bb-49f2-9331-45f3694ce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(X, y, sample_size, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    classes, class_counts = np.unique(y, return_counts=True)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Calculate target samples per class while accounting for class sizes\n",
    "    total_samples = len(y)\n",
    "    class_ratios = class_counts / total_samples\n",
    "    target_samples_per_class = np.floor(sample_size * class_ratios).astype(int)\n",
    "\n",
    "    # Distribute any remaining samples\n",
    "    remaining_samples = sample_size - np.sum(target_samples_per_class)\n",
    "    if remaining_samples > 0:\n",
    "        sorted_class_indices = np.argsort(class_counts)[::-1]\n",
    "        for idx in sorted_class_indices:\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "            available_samples = class_counts[idx] - target_samples_per_class[idx]\n",
    "            samples_to_add = min(remaining_samples, available_samples)\n",
    "            target_samples_per_class[idx] += samples_to_add\n",
    "            remaining_samples -= samples_to_add\n",
    "\n",
    "    # Collect samples from each class\n",
    "    indices = []\n",
    "    for i, cls in enumerate(classes):\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        n_samples = min(target_samples_per_class[i], len(cls_indices))\n",
    "        sampled_indices = np.random.choice(cls_indices, size=n_samples, replace=False)\n",
    "        indices.extend(sampled_indices)\n",
    "\n",
    "    # Shuffle\n",
    "    indices = np.array(indices)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860b9e2-2c23-4145-beb7-3f88df20314e",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "*Reference:* https://towardsdatascience.com/implement-multiclass-svm-from-scratch-in-python-b141e43dc084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93a26c0-a005-49f1-808c-6667a87d1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "class LabelBinarizer:\n",
    "    def __init__(self):\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        Y = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for i, cls in enumerate(self.classes_):\n",
    "            Y[:, i] = (y == cls)\n",
    "        return Y\n",
    "\n",
    "    def inverse_transform(self, Y):\n",
    "        return self.classes_[np.argmax(Y, axis=1)]\n",
    "\n",
    "\n",
    "class BinarySVM:\n",
    "    def __init__(self, kernel, C=1, tol=1e-3, max_iter=1000, class_weight=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.class_weight = class_weight\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "\n",
    "    def _compute_sample_weights(self, y):\n",
    "        if self.class_weight is None:\n",
    "            return np.ones(len(y))\n",
    "        elif self.class_weight == 'balanced':\n",
    "            # Compute balanced weights\n",
    "            unique_classes = np.unique(y)\n",
    "            class_weights = {}\n",
    "            n_samples = len(y)\n",
    "            for cls in unique_classes:\n",
    "                class_weights[cls] = n_samples / (len(unique_classes) * np.sum(y == cls))\n",
    "        else:\n",
    "            class_weights = self.class_weight\n",
    "\n",
    "        sample_weights = np.ones(len(y))\n",
    "        for cls, weight in class_weights.items():\n",
    "            sample_weights[y == cls] = weight\n",
    "        return sample_weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.b = 0\n",
    "        sample_weights = self._compute_sample_weights(y)\n",
    "        K = self.kernel(X, X)\n",
    "        y = y.astype(float)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            alpha_prev = self.alpha.copy()\n",
    "            predictions = np.dot(K, self.alpha * y) + self.b\n",
    "            margins = y * predictions\n",
    "            mask = margins < 1\n",
    "            updates = np.zeros(n_samples)\n",
    "            updates[mask] = self.C * sample_weights[mask] * (1 - margins[mask])\n",
    "            self.alpha += updates\n",
    "            self.alpha = np.minimum(np.maximum(self.alpha, 0), self.C * sample_weights)\n",
    "            weight_sum = np.sum(sample_weights)\n",
    "            self.b = np.sum(sample_weights * (y - np.dot(K, self.alpha * y))) / weight_sum\n",
    "            diff = np.linalg.norm(self.alpha - alpha_prev)\n",
    "            \n",
    "            if diff < self.tol:\n",
    "                print(f\"Converged in {iteration} iterations\")\n",
    "                break\n",
    "\n",
    "        # Store support vectors\n",
    "        sv_mask = self.alpha > 1e-5\n",
    "        self.support_vectors = X[sv_mask]\n",
    "        self.support_vector_labels = y[sv_mask]\n",
    "        self.alpha = self.alpha[sv_mask]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.support_vectors is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        K = self.kernel(X, self.support_vectors)\n",
    "        return np.sign(np.dot(K, self.alpha * self.support_vector_labels) + self.b)\n",
    "\n",
    "\n",
    "class MulticlassSVM:\n",
    "    def __init__(self, kernel='linear', C=1, tol=1e-3, max_iter=500, class_weight=None):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel = self._get_kernel(kernel)\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.class_weight = class_weight\n",
    "        self.label_binarizer = LabelBinarizer()\n",
    "        self.classifiers = []\n",
    "\n",
    "    def _get_kernel(self, kernel_name):\n",
    "        if kernel_name == 'linear':\n",
    "            return lambda x, y: np.dot(x, y.T)\n",
    "        elif kernel_name == 'polynomial':\n",
    "            return lambda x, y, Q=3: (1 + np.dot(x, y.T)) ** Q\n",
    "        elif kernel_name == 'rbf':\n",
    "            return lambda x, y, γ=1: np.exp(-γ * cdist(x, y, 'sqeuclidean'))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid kernel: {kernel_name}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to binary problems all at once\n",
    "        y_binary = self.label_binarizer.fit_transform(y)\n",
    "        n_classes = y_binary.shape[1]\n",
    "\n",
    "        # Calculate class weights for multiclass case if 'balanced'\n",
    "        if self.class_weight == 'balanced':\n",
    "            n_samples = len(y)\n",
    "            class_weights = {}\n",
    "            for i, cls in enumerate(self.label_binarizer.classes_):\n",
    "                class_weights[1] = n_samples / (2 * np.sum(y == cls))  # weight for positive class\n",
    "                class_weights[-1] = n_samples / (2 * np.sum(y != cls))  # weight for negative class\n",
    "        else:\n",
    "            class_weights = self.class_weight\n",
    "\n",
    "        # Train all binary classifiers\n",
    "        self.classifiers = []\n",
    "        for i in range(n_classes):\n",
    "            print(f\"Training classifier for class {i}\")\n",
    "            clf = BinarySVM(\n",
    "                kernel=self.kernel,\n",
    "                C=self.C,\n",
    "                tol=self.tol,\n",
    "                max_iter=self.max_iter,\n",
    "                class_weight=class_weights\n",
    "            )\n",
    "            clf.fit(X, 2 * y_binary[:, i] - 1)  # Convert to {-1, 1}\n",
    "            self.classifiers.append(clf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classifiers)\n",
    "        predictions = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for i, clf in enumerate(self.classifiers):\n",
    "            predictions[:, i] = clf.predict(X)\n",
    "\n",
    "        return self.label_binarizer.inverse_transform(predictions > 0)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efb9fd-e3f7-4d5f-8464-5f05690592ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Testing hyperparameters...\n",
      "Testing with hyperparameters: {'kernel': 'linear', 'C': 0.1, 'tol': 0.0001, 'max_iter': 1000}\n",
      "Training classifier for class 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from svm2 import MulticlassSVM\n",
    "\n",
    "\n",
    "\n",
    "def stratified_sampling(X, y, sample_percentage, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    classes, class_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    indices = []\n",
    "    for cls in classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        # Calculate the number of samples for the current class\n",
    "        cls_sample_count = max(1, int(len(cls_indices) * sample_percentage))\n",
    "        # Sample indices for the current class\n",
    "        sampled_indices = np.random.choice(cls_indices, size=cls_sample_count, replace=False)\n",
    "        indices.extend(sampled_indices)\n",
    "\n",
    "    indices = np.array(indices, dtype=int)  # Ensure indices are integers\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "\n",
    "def test_hyperparameters(X_train, y_train, X_val, y_val, hyperparams):\n",
    "    results = []\n",
    "    for params in hyperparams:\n",
    "        print(f\"Testing with hyperparameters: {params}\")\n",
    "\n",
    "        # Perform sampling if sampling percentage is specified\n",
    "        if 'sample_percentage' in params:\n",
    "            X_sampled, y_sampled = stratified_sampling(X_train, y_train, params['sample_percentage'])\n",
    "        else:\n",
    "            X_sampled, y_sampled = X_train, y_train\n",
    "\n",
    "        # Initialize the model with given hyperparameters\n",
    "        model = MulticlassSVM(kernel=params['kernel'], C=params['C'], tol=params['tol'], max_iter=params['max_iter'])\n",
    "        model.fit(X_sampled, y_sampled)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        results.append({'params': params, 'accuracy': accuracy})\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    hyperparams = [res['params'] for res in results]\n",
    "    accuracy_score = [res['accuracy'] for res in results]\n",
    "\n",
    "    x_labels = [\n",
    "        f\"{params['kernel'].upper()} (C={params['C']})\" for params in hyperparams\n",
    "    ]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_labels, accuracy_score, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "    plt.title('SVM Hyperparameter Performance', fontsize=16)\n",
    "    plt.xlabel('Hyperparameter Configuration', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    for i, accuracy in enumerate(accuracy_score):\n",
    "        plt.text(i, accuracy, f'{accuracy}%',\n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "X = np.array(train_data['images']).reshape(len(train_data['images']), -1)\n",
    "y = np.array(train_data['labels'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "hyperparams = [\n",
    "    {'kernel': 'linear', 'C': 0.1, 'tol': 1e-4, 'max_iter': 1000, 'sample_percentage': 0.3},\n",
    "    {'kernel': 'linear', 'C': 1.0, 'tol': 1e-4, 'max_iter': 1000, 'sample_percentage': 0.3},\n",
    "    {'kernel': 'rbf', 'C': 1.0, 'tol': 1e-3, 'max_iter': 2000, 'sample_percentage': 0.3},\n",
    "    {'kernel': 'rbf', 'C': 10.0, 'tol': 1e-4, 'max_iter': 2000, 'sample_percentage': 0.3}\n",
    "]\n",
    "\n",
    "print(\"Testing hyperparameters...\")\n",
    "results = test_hyperparameters(X_train, y_train, X_val, y_val, hyperparams)\n",
    "print(\"Plotting results...\")\n",
    "plot_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287c550bf162055",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09eaf99f-52a3-4035-893d-c0b67dac3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a51eb604f492a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self, input_shape, num_classes, lr=0.01, epochs=50, batch_size=32):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.params = self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        np.random.seed(42)\n",
    "        filter_size = 3\n",
    "        num_filters = 8\n",
    "        hidden_units = 64\n",
    "\n",
    "        # Calculate the size of the flattened layer after pooling\n",
    "        conv_output_height = self.input_shape[0] - filter_size + 1  # After convolution\n",
    "        conv_output_width = self.input_shape[1] - filter_size + 1\n",
    "        pool_output_height = conv_output_height // 2  # After pooling\n",
    "        pool_output_width = conv_output_width // 2\n",
    "        flattened_size = num_filters * pool_output_height * pool_output_width\n",
    "\n",
    "        weights = {\n",
    "            \"conv_filters\": np.random.randn(num_filters, filter_size, filter_size) * 0.1,\n",
    "            \"conv_bias\": np.zeros(num_filters),\n",
    "            \"fc_weights\": np.random.randn(flattened_size, hidden_units) * 0.1,\n",
    "            \"fc_bias\": np.zeros(hidden_units),\n",
    "            \"out_weights\": np.random.randn(hidden_units, self.num_classes) * 0.1,\n",
    "            \"out_bias\": np.zeros(self.num_classes)\n",
    "        }\n",
    "        return weights\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def _conv_forward(self, X):\n",
    "        n_samples, height, width = X.shape\n",
    "        num_filters, filter_size, _ = self.params[\"conv_filters\"].shape\n",
    "        output_size = height - filter_size + 1\n",
    "        conv_output = np.zeros((n_samples, num_filters, output_size, output_size))\n",
    "\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                region = X[:, i:i + filter_size, j:j + filter_size]\n",
    "                conv_output[:, :, i, j] = np.tensordot(region, self.params[\"conv_filters\"], axes=([1, 2], [1, 2])) + self.params[\"conv_bias\"]\n",
    "        return self._relu(conv_output)\n",
    "\n",
    "    def _pool_forward(self, X):\n",
    "        n_samples, num_filters, height, width = X.shape\n",
    "        output_size = height // 2\n",
    "        pool_output = np.zeros((n_samples, num_filters, output_size, output_size))\n",
    "\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                region = X[:, :, i * 2:i * 2 + 2, j * 2:j * 2 + 2]\n",
    "                pool_output[:, :, i, j] = np.max(region, axis=(2, 3))\n",
    "        return pool_output\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "\n",
    "    def _fc_forward(self, X, weights, bias):\n",
    "        return np.dot(X, weights) + bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(self.epochs):\n",
    "            shuffled_indices = np.random.permutation(n_samples)\n",
    "            X_shuffled, y_shuffled = X[shuffled_indices], y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                conv_output = self._conv_forward(X_batch)\n",
    "                pool_output = self._pool_forward(conv_output)\n",
    "                flat_output = self._flatten(pool_output)\n",
    "                fc_output = self._relu(self._fc_forward(flat_output, self.params[\"fc_weights\"], self.params[\"fc_bias\"]))\n",
    "                logits = self._fc_forward(fc_output, self.params[\"out_weights\"], self.params[\"out_bias\"])\n",
    "                probs = self._softmax(logits)\n",
    "\n",
    "                y_one_hot = np.eye(self.num_classes)[y_batch]\n",
    "                loss = -np.sum(y_one_hot * np.log(probs + 1e-8)) / self.batch_size\n",
    "\n",
    "                grad_logits = (probs - y_one_hot) / self.batch_size\n",
    "                grad_fc_weights_out = np.dot(fc_output.T, grad_logits)\n",
    "                grad_fc_bias_out = np.sum(grad_logits, axis=0)\n",
    "\n",
    "                grad_fc_output = np.dot(grad_logits, self.params[\"out_weights\"].T) * self._relu_derivative(fc_output)\n",
    "                grad_fc_weights = np.dot(flat_output.T, grad_fc_output)\n",
    "                grad_fc_bias = np.sum(grad_fc_output, axis=0)\n",
    "\n",
    "                self.params[\"fc_weights\"] -= self.lr * grad_fc_weights\n",
    "                self.params[\"fc_bias\"] -= self.lr * grad_fc_bias\n",
    "                self.params[\"out_weights\"] -= self.lr * grad_fc_weights_out\n",
    "                self.params[\"out_bias\"] -= self.lr * grad_fc_bias_out\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        conv_output = self._conv_forward(X)\n",
    "        pool_output = self._pool_forward(conv_output)\n",
    "        flat_output = self._flatten(pool_output)\n",
    "        fc_output = self._relu(self._fc_forward(flat_output, self.params[\"fc_weights\"], self.params[\"fc_bias\"]))\n",
    "        logits = self._fc_forward(fc_output, self.params[\"out_weights\"], self.params[\"out_bias\"])\n",
    "        probs = self._softmax(logits)\n",
    "\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ad49ad-2d07-4dbc-96ec-0f9d8ba6a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callCNN(X_train, y_train, X_test):\n",
    "    # Initialize and train CNN\n",
    "    print(\"Training model...\")\n",
    "    cnn = SimpleCNN(input_shape=(28, 28), num_classes=4, lr=0.01, epochs=5, batch_size=32)\n",
    "    cnn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_test_pred = cnn.predict(X_test)\n",
    "\n",
    "    train_accuracy = cnn.score(X_train, y_train)\n",
    "    print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save predictions\n",
    "    save('cnn_submission.csv', y_test_pred)\n",
    "\n",
    "def analyze_epochs(X_train, y_train, X_val, y_val, epoch_range=range(1, 16)):\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epochs in epoch_range:\n",
    "        print(f\"Training with {epochs} epochs...\")\n",
    "\n",
    "        # Create and train model\n",
    "        cnn = SimpleCNN(input_shape=(28, 28), num_classes=4,\n",
    "                        lr=0.01, epochs=epochs, batch_size=32)\n",
    "        cnn.fit(X_train, y_train)\n",
    "\n",
    "        # Compute accuracies\n",
    "        train_acc = cnn.score(X_train, y_train)\n",
    "        val_acc = cnn.score(X_val, y_val)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "    return train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "def plot_epoch_performance(train_accuracies, val_accuracies, epoch_range = range(10, 13)):\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epoch_range, train_accuracies, marker='o', label='Training Accuracy')\n",
    "    plt.plot(epoch_range, val_accuracies, marker='o', label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Number of Epochs')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af8eeb3-cf59-47d0-999d-eb5a5b2bbfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preparing data...\n",
      "Training with 1 epochs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Analyze epochs\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m analyze_epochs(X_train, y_train, X_val, y_val)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     19\u001b[0m plot_epoch_performance(train_accuracies, val_accuracies)\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36manalyze_epochs\u001b[0;34m(X_train, y_train, X_val, y_val, epoch_range)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create and train model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m cnn \u001b[38;5;241m=\u001b[39m SimpleCNN(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     26\u001b[0m                 lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m cnn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute accuracies\u001b[39;00m\n\u001b[1;32m     30\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m cnn\u001b[38;5;241m.\u001b[39mscore(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m, in \u001b[0;36mSimpleCNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m pool_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_forward(conv_output)\n\u001b[1;32m     84\u001b[0m flat_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten(pool_output)\n\u001b[0;32m---> 85\u001b[0m fc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fc_forward(flat_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     86\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fc_forward(fc_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     87\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_softmax(logits)\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mSimpleCNN._fc_forward\u001b[0;34m(self, X, weights, bias)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fc_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, weights, bias):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(X, weights) \u001b[38;5;241m+\u001b[39m bias\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    X = np.array(train_data['images']).reshape(len(train_data['images']), 28, 28)\n",
    "    y = np.array(train_data['labels'])\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Analyze epochs\n",
    "    train_accuracies, val_accuracies = analyze_epochs(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Plot results\n",
    "    plot_epoch_performance(train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print best epoch\n",
    "    best_epoch = val_accuracies.index(max(val_accuracies)) + 1\n",
    "    print(f\"\\nBest number of epochs: {best_epoch}\")\n",
    "    print(f\"Best validation accuracy: {max(val_accuracies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79310c-2b90-4703-86aa-f8ad196489f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
